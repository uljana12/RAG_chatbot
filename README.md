# Copenhagen IT Job Search Assistant

A production-ready **Retrieval-Augmented Generation (RAG)** platform built with LangChain, helping job seekers find IT positions in Copenhagen, Denmark.

> **Purpose**: AI-powered assistant for Copenhagen IT job search  
> **Architecture**: Modern RAG pipeline with conversation memory  
> **LLM**: Ollama with Llama 3.2 (free, runs locally)

## ğŸ¯ Key Features

| Feature | Technology | Description |
|---------|------------|-------------|
| **RAG Pipeline** | LangChain | Document ingestion, chunking, retrieval with source deduplication |
| **Vector Store** | ChromaDB (Persistent) | Durable storage with singleton client pattern |
| **LLM Backend** | Ollama (Llama 3.2) | Free local LLM, no API key needed |
| **Embeddings** | langchain_ollama | Local embeddings generation |
| **Web UI** | Streamlit | Modern chat interface with Inter font |
| **REST API** | FastAPI | Production API with OpenAPI docs |
| **Containerization** | Docker Compose | One-command deployment |
| **Observability** | Prometheus/Grafana | Metrics and monitoring |

## ğŸ“ Project Structure

```
LLM_Example/
â”œâ”€â”€ app.py                      # Streamlit Web UI
â”œâ”€â”€ Dockerfile                  # Multi-stage production build
â”œâ”€â”€ docker-compose.yml          # Full stack deployment
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Configuration
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingestion.py           # Document processing & ChromaDB storage
â”‚   â”œâ”€â”€ rag_chain.py           # RAG chain with conversation memory
â”‚   â”œâ”€â”€ job_scraper.py         # Copenhagen IT job listings (mock data)
â”‚   â””â”€â”€ api.py                 # FastAPI REST API endpoints
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ARCHITECTURE.md        # C4 diagrams & technical docs
â”‚
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus.yml         # Metrics configuration
â”‚
â””â”€â”€ data/
    â””â”€â”€ sample_knowledge_base.txt
```

## ğŸ› ï¸ Setup

### 1. Create Virtual Environment

```bash
cd LLM_Example
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Install Ollama (Local LLM - FREE)

```bash
# macOS
brew install ollama
brew services start ollama

# Pull the model
ollama pull llama3.2
```

### 4. Configure Environment (Optional)

```bash
cp .env.example .env
```

Default configuration uses Ollama (free, local). No API key needed!

> **Note**: Data is stored persistently in the `chroma_db/` folder.

### 5. Run the Application

**Option A: Local (Development)**
```bash
source venv/bin/activate
streamlit run app.py --server.headless true
```

**Option B: Docker (Production)**
```bash
docker build -t rag-chatbot .
docker run -p 8501:8501 -e OLLAMA_BASE_URL=http://host.docker.internal:11434 rag-chatbot
```

The app will open in your browser at `http://localhost:8501`

## ğŸ“– Usage

### Adding Data

1. **Text Input**: Paste text directly into the sidebar
2. **URLs**: Enter web URLs to scrape and index
3. **File Upload**: Upload PDF, TXT, or MD files

### Chatting

Once you've added data, simply type your questions in the chat input. The chatbot will:
1. Search your knowledge base for relevant information
2. Use the retrieved context to generate an accurate response
3. Show source documents used for the answer

### Example Queries

After loading the sample data:
- "What products does TechCorp offer?"
- "How much does SmartAssist AI cost?"
- "What is the refund policy?"
- "How do I contact customer support?"

## ğŸ”§ Configuration

### Customizing the RAG Pipeline

Edit `src/rag_chain.py` to adjust:
- `temperature`: Controls response creativity (0.0 - 1.0)
- `k`: Number of documents to retrieve (default: 4)
- `DEFAULT_SYSTEM_PROMPT`: The instructions given to the LLM

### Customizing Document Processing

Edit `src/ingestion.py` to adjust:
- `CHUNK_SIZE`: Size of text chunks (default: 1000)
- `CHUNK_OVERLAP`: Overlap between chunks (default: 200)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚â”€â”€â”€â”€â–¶â”‚    Retriever     â”‚â”€â”€â”€â”€â–¶â”‚  Vector Store   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (Similarity     â”‚     â”‚   (ChromaDB)    â”‚
                        â”‚   Search)        â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Retrieved Docs   â”‚
                        â”‚ (Top K matches)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Response     â”‚â—€â”€â”€â”€â”€â”‚ LLM (Ollama/     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Llama 3.2)       â”‚
                        â”‚ + Context + Queryâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ API Usage

### REST API (FastAPI)

Start the API server:
```bash
uvicorn src.api:app --reload --port 8000
```

Interactive documentation at: http://localhost:8000/docs

**Example API calls:**

```bash
# Health check
curl http://localhost:8000/health

# Load job data
curl -X POST http://localhost:8000/ingest/jobs

# Chat
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "What Python jobs are available?"}'

# Get metrics
curl http://localhost:8000/metrics
```

### Python SDK

```python
from src.ingestion import ingest_text, load_vector_store
from src.rag_chain import RAGChatbot
from src.job_scraper import get_copenhagen_it_jobs

# Load Copenhagen IT job listings
job_data = get_copenhagen_it_jobs()
ingest_text(job_data, "copenhagen_jobs")

# Or add custom content
ingest_text("Your custom content here...", "my_source")

# Create chatbot instance with vector store
vector_store = load_vector_store()
chatbot = RAGChatbot(vector_store=vector_store)

# Ask questions (sources are automatically deduplicated)
result = chatbot.chat("What Python jobs are available?", include_sources=True)
print(result["answer"])
for source in result["sources"]:
    print(f"  - {source['source']}")
```

## ğŸ³ Docker Deployment

### Quick Start (Docker Compose)

```bash
# Start all services (Web UI + API + Ollama)
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

Services:
- **Web UI**: http://localhost:8501
- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

### With Monitoring

```bash
# Include Prometheus + Grafana
docker-compose --profile monitoring up -d
```

- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)

## ğŸ“Š Architecture Documentation

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for:
- C4 diagrams (Context, Container, Component)
- Data flow diagrams
- Deployment architecture
- Security considerations
- Technology decisions

## ğŸ”§ Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_MODEL` | llama3.2 | Local LLM model |
| `OLLAMA_BASE_URL` | http://localhost:11434 | Ollama service URL |

## ğŸ¤ Contributing

Feel free to submit issues and pull requests!

## ğŸ“„ License

MIT License - feel free to use this for learning and personal projects.
